{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IsBit Research and Testing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Plan\n",
    "\n",
    "The initial plan is to split the data set of 5451 question text data into three separate subsets.\n",
    "The formatting of each data set is described below. The first set contains 80% of the parent data and is reserved for local training of different\n",
    "clustering models. The remaining 20% are further divided into two unseen test sets of 10% each, one for remote testing exported to the application and another for local evaluation of models(hence containing the coarse-label).\n",
    "\n",
    "`ML/data/QAQC_v1/swe_local_train.csv`: \n",
    "\n",
    " This is the data source that is preprocessed and split into three subsets.\n",
    "\n",
    "`ML/data/QAQC_v1/splits/local_train.csv `:\n",
    "\n",
    "This is the first subset, where data rows are formatted as `Hur utvecklades träldomen i Ryssland?`. These are the questions used for training the local clustering models.\n",
    "\n",
    "`ML/data/QAQC_v1/splits/local_test.csv`: \n",
    "\n",
    "This is the second subset formatted as `Hur utvecklades träldomen i Ryssland?, DESC`. These are questions with additional coarse labels used for local evaluation of the different clustering models.\n",
    "\n",
    "`ML/data/QAQC_v1/splits/remote_test.csv` :\n",
    "\n",
    "This is the third subset with the same formatting as the second one. These questions are meant to be used for user-tests exported to the application.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAQC Preprocessing\n",
    "\n",
    "The following section will preprocess and split the source data into the three mentioned data slices, keep in mind that all data file names are in the .gitignore to avoid pushing data files to origin, download the source data set to the following path `ML/data/QAQC_v1` and run the preprocessing section to slice the data for training and testing. The souce data can be found at `https://spraakbanken.gu.se/en/resources/sveat` \n",
    "\n",
    "If further preprocessing is needed I suggest saving it under `ML/data/QAQC_v2/` and inserting the created file names into .gitignore.\n",
    "\n",
    "The cells below will filter the source data and create new formatted csv files with the following changes in format compared to the source data,\n",
    "\n",
    "- Removing commas inside the text field replacing then with pipe signs | .\n",
    "- Removing outer quotations marks of the textfield.\n",
    "- Insert escape chars where needed, mostly text fields containing quotes. \n",
    "\n",
    "Examples of both raw and processed data files with some normal and edge cases in the data rows can be found under `ML/data_exampels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run\n",
    "# pip install -r ML/requirements.txt\n",
    "\n",
    "# or run these pip intall \n",
    "# > pip install scikit-learn\n",
    "# > pip install -U sentence-transformers\n",
    "# > pip install seaborn\n",
    "\n",
    "# or uncomment lines below and run this cell.\n",
    "#'%pip install scikit-learn'\n",
    "#'%pip install sentence-transformers'\n",
    "#'%pip install seaborn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "exists = os.path.isfile(os.path.join(\"./data/QAQC_v1\", \"swe_qaqc_train.csv\"))\n",
    "if exists:\n",
    "    print(\"File exists\")\n",
    "else:\n",
    "    dataset = \"https://svn.spraakbanken.gu.se/sb-arkiv/pub/trec/swe_qaqc_train.csv\"\n",
    "    response = requests.get(dataset)\n",
    "    open(\"./data/QAQC_v1/swe_qaqc_train.csv\", \"wb\").write(response.content)\n",
    "    print(\"File downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing of commas and quotations marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "source_path = \"data/QAQC_v1/swe_qaqc_train.csv\"        # path to the source data, need to be downloaded ignored for git\n",
    "output_path = \"data/QAQC_v1/swe_qaqc_prep_train.csv\"   # set the same output path to not push data to git\n",
    "\n",
    "# helper to remove outer quotation marks\n",
    "def strip_outer_quotationmarks(q):\n",
    "    if q.startswith('\"') and q.endswith('\"'):\n",
    "        q = q[1:-1]  \n",
    "    return q.strip()\n",
    "\n",
    "# helper to remove commas in the question text, causes problems since the cols are set by commas in csv file\n",
    "def replace_commas(line):\n",
    "    pattern = r'\"([^\"]*?)\"'\n",
    "    def replace_commas(match):\n",
    "        return '\"' + match.group(1).replace(',', '|') + '\"'\n",
    "    return re.sub(pattern, replace_commas, line)\n",
    "\n",
    "def process_csv(source_data_path, output_data_path, temp_file_path=\"temp_file.csv\"):\n",
    "    with open(source_data_path, 'r', encoding='utf-8') as infile:\n",
    "        modified_lines = [replace_commas(line.strip()) for line in infile]\n",
    "\n",
    "    with open(temp_file_path, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        outfile.write('\\n'.join(modified_lines))\n",
    "\n",
    "    temp_data = pd.read_csv(temp_file_path, quoting=csv.QUOTE_NONE)  \n",
    "    questions = temp_data[\"text\"].tolist()\n",
    "    no_comma_questions = [strip_outer_quotationmarks(q) for q in questions]\n",
    "\n",
    "    coarse_labels = list(map(lambda x: x.split(\":\")[0], temp_data[\"verbose label\"].tolist())) # only keep the coarse lable \n",
    "    zipped = list(zip(no_comma_questions, coarse_labels))\n",
    "\n",
    "    with open(output_data_path, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        writer = csv.writer(outfile, quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([\"text\", \"coarse label\"])\n",
    "        writer.writerows(zipped)\n",
    "\n",
    "    if os.path.exists(temp_file_path):\n",
    "        os.remove(temp_file_path)\n",
    "\n",
    "\n",
    "    #print(f\"Processed data saved to {output_data_path}\") # trace print\n",
    "\n",
    "# data formatted and ready for splitting \n",
    "_ = process_csv(source_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QAQC data split Version 1\n",
    "\n",
    "Splits the formatted data into 80% local training data, 10% local test data and 10% remote user test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_split(source_data_path, split_path):\n",
    "    \n",
    "    data = pd.read_csv(source_data_path)\n",
    "    os.makedirs(split_path, exist_ok=True)\n",
    "    \n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=True, stratify=data['coarse label'])\n",
    "    local_test_data, remote_test_data = train_test_split(test_data, test_size=0.5, random_state=42, shuffle=True, stratify=test_data['coarse label'])\n",
    "    \n",
    "    local_train_data = train_data[['text']].copy()  # no need for the lable that wont be used as a lable\n",
    "    \n",
    "    local_test_data_for_eval = local_test_data[['text', 'coarse label']].copy()\n",
    "    remote_test_data_for_eval = remote_test_data[['text', 'coarse label']].copy()\n",
    "    \n",
    "    local_train_file = os.path.join(split_path, \"local_train.csv\")\n",
    "    local_test_file = os.path.join(split_path, \"local_test.csv\")\n",
    "    remote_test_file = os.path.join(split_path, \"remote_test.csv\")\n",
    "    \n",
    "    local_train_data.to_csv(local_train_file, index=False, header=True)\n",
    "    local_test_data_for_eval.to_csv(local_test_file, index=False, header=True)\n",
    "    remote_test_data_for_eval.to_csv(remote_test_file, index=False, header=True)\n",
    "    \n",
    "    print(f\"Data splits saved to: {split_path}\")\n",
    "\n",
    "\n",
    "source_path = \"data/QAQC_v1/swe_qaqc_prep_train.csv\"\n",
    "split_path = \"data/QAQC_v1/splits/\"\n",
    "data_split(source_path, split_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QAQC data split version 2\n",
    "\n",
    "Splits the formatted data into 90% training data and 10% for late stage validation.\n",
    "\n",
    "Creates two copies of the 90% sized set, one without the coarse label and one with the coarse label. The data set **without** the coarse label should be used when creating data points to be sent forward in the application pipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def data_split(source_data_path, split_path):\n",
    "    \n",
    "    data = pd.read_csv(source_data_path)\n",
    "    os.makedirs(split_path, exist_ok=True)\n",
    "    \n",
    "    train_data, test_data = train_test_split(data, test_size=0.1, random_state=42, shuffle=True, stratify=data['coarse label'])\n",
    "    \n",
    "    train_data_nl = train_data[['text']].copy()\n",
    "    train_data_wl = train_data[['text', 'coarse label']].copy()  \n",
    "    test_data = test_data [['text', 'coarse label']].copy()\n",
    "    \n",
    "    train_nl_file = os.path.join(split_path, \"train_nl_data.csv\")\n",
    "    train_wl_file = os.path.join(split_path, \"train_wl_data.csv\")\n",
    "    test_file  = os.path.join(split_path, \"test_data.csv\")\n",
    "    \n",
    "    train_data_nl.to_csv(train_nl_file, index=False, header=True)\n",
    "    train_data_wl.to_csv(train_wl_file, index=False, header=True)\n",
    "    test_data.to_csv(test_file, index=False, header=True)\n",
    "\n",
    "    print(f\"Data splits saved to: {split_path}\")\n",
    "\n",
    "\n",
    "source_path = \"data/QAQC_v1/swe_qaqc_prep_train.csv\"\n",
    "split_path = \"data/QAQC_v2/splits/\"\n",
    "data_split(source_path, split_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SentenceTransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing pre trained model referenced on official page of SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "train_data_path = 'data/QAQC_v2/splits/train_nl_data.csv'\n",
    "data = pd.read_csv(train_data_path)\n",
    "sentences = data['text'].tolist() \n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "num_clusters = 6 # coarse lables: [LOC, HUM, DESC, ENTY, ABBR, NUM] \n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(embeddings.cpu().numpy())\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# reduction\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings.cpu().numpy())\n",
    "\n",
    "# frame\n",
    "cluster_df = pd.DataFrame(reduced_embeddings, columns=['x', 'y'])\n",
    "\n",
    "print(cluster_df)\n",
    "\n",
    "cluster_df['cluster'] = clusters\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=cluster_df, x='x', y='y', hue='cluster', palette='viridis', alpha=0.6)\n",
    "plt.title('Sentence Clustering Visualization')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Clusters')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
